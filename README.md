# LLM-Analysis
### Project Description
The prototype aims to support the analysis of texts through the use of machine learning models, in particular LLM models (Large Language Models).
The purpose of the prototype is to analyze a CSV file containing a series of texts (e.g. abstracts of scientific papers) from a set of default queries automatically executed by the LLM model. Questions may concern keyword extraction requests, summarization, degree of consistency with a specific topic, etc. The results of the queries will be saved to a NoSQL database.
### Prototype functionalities:
- Read data from existing JSON (or CSV) files
- Read a list of questions
- Interatively submit the question on each element of the input
- Save the question results into the database
---
### Prototype design
![image](https://github.com/Doan314/LLM-Analysis/assets/160939362/d1b18d4a-e56f-4dff-bd4a-c246a4deb08c)
- Data and question files are loaded to **MongoDB** - a NoSQL database
- The data file is read by the LLM protytype, which uses
  - **Anaconda**: A Python package and environment management environment that simplifies the installation and management of dependencies.
  - **PyTorch**: An open-source machine learning framework that provides capabilities for the creation and training of neural networks.
  - **LLAMA-CPP-Pytho**n: A library that provides easy access and use to PyTorch-based Large Language Models.
  - **CUDA**: a parallel computing platform developed by NVIDIA, which enables the use of GPUs for computing operations.- Onprem: A Python library to run Large Language Model locally.
  - **Jupyter Notebook**: A development tool that supports different programming languages such as Python.
 to generate the answer.
- The answer generated by the LLM will be saved into MongoDB in JSON format, using **Simple File Connector**.
#### Access MongoDB database
To connect to the database, it is best to download the [MongoDb Compass tool](https://www.mongodb.com/products/tools/compass) from the manufacturer's official website. Once installed, enter the given URI:
mongodb+srv://bigdata:ProjectBigData123@llm.gsegixp.mongodb.net/
Then save the database name under any name of your choice and press connect.
> [!NOTE]  
> The database does not work with UnicamEasyWifi. Use a different internet connection.
#### Implementation Large Language Model
*The following instructions refer to an installation with Windows*
STEP1: installed the requirements packages for Onprem within Anaconda:
- Donwload for Anaconda: (https://docs.anaconda.com/free/anaconda/install/windows/)
  created environment LLM in Anaconda
  - CUDA to take advantage of the performance of NVIDIA GPUs (cuBlas library)
    (https://developer.nvidia.com/cuda-downloads)
- PyTorch  (https://pytorch.org/get-started/locally/)
    command: `conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia`
- llama-cpp-python (https://python.langchain.com/docs/integrations/llms/llamacpp#installation)
  set CMAKE_ARGS=-DLLAMA_CUBLAS=on
  set FORCE_CMAKE=1  
  pip install llama-cpp-python

STEP2: installed Onprem (https://github.com/amaiya/onprem)
  command: pip install onprem

STEP3: check if Jupyter is installed in Anaconda
- Select LLM kernel

STEP4: Speeding Up Inference Using a GPU (https://python.langchain.com/docs/integrations/llms/llamacpp)
  Modified two of parameters from the core.py file:
  n_gpu_layers → how many layers of the model are downloaded to the GPU
 n_gpu_layers: int = 24
  n_batch → how many tokens are processed in parallel
 n_batch: int = 800

STEP5: adapt the model to the proposed problem
  - check if the langchain package is installed
  - since the model has to answer one item at a time I create a loop that iterates my sequence of
    documents, for each one the call self.combine_documents_chain.run() is executed and added the  
    result to the answer list.
    from langchain\chains\retrieval_qa\base.py, _call() function:
    answer = []
        for document in docs:
            answer.append(self.combine_documents_chain.run(
                input_documents=document, question=question, callbacks=_run_manager.get_child()
            )
            )

STEP6: "Talk to Your Documents" section of Onprem
    - created New Notebook (Run.ipynb file)
    - passed the file CSV containing the articles and ingests document in source_directory
    - passed the file JSON containing the questions
    - used the "ask" method, for each question an answer is returned
   for question in all_question:
      result_question = llm.ask(question)

Note: The LLM template processes requests in parallel and therefore the order in which the answers are provided could correspond to the original order of the items.

STEP8: result saved in a json file

STEP9: send the Data in a JSON file to the mongodb database
  - mongodb_uri = "mongodb+srv://bigdata:ProjectBigData123@llm.gsegixp.mongodb.net/"  
  - database_name = "BigData"
  - collection_name = "Output"
